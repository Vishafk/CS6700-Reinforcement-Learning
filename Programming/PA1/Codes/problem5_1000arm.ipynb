{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and packages\n",
    "\n",
    "import numpy as np\n",
    "from random import randint, uniform\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redefine epsilon-greedy, softmax ucb with 50000 time steps for comparing with MEA on the 1000 arm problem###\n",
    "\n",
    "\n",
    "\n",
    "### EPSILON-GREEDY....###\n",
    "\n",
    "\n",
    "\n",
    "def epsilon_greedy(n_arms, epsilon):\n",
    "  \n",
    "  ''' Function to implement the epsilon-greedy approach for the bandit problem '''\n",
    "\n",
    "  # Mean of each of the arms selected from a normal distrubtion of mean 0, variance 1\n",
    "  means = np.random.normal(0,1,n_arms)  \n",
    "\n",
    "  # Arm with maximum mean or reward (used to derive % Optimality)\n",
    "  max_index = means.argmax()  \n",
    "\n",
    "  # Initialize parameters \n",
    "  Q = np.zeros(n_arms)  \n",
    "  N = np.ones(n_arms)                   \n",
    "\n",
    "  ### Randomly pick an arm ###\n",
    "  arm_index = randint(0, n_arms-1)            \n",
    "  Q[arm_index] = np.random.normal(means[arm_index], 1, 1)[0]\n",
    "\n",
    "  # List to keep track of all the rewards\n",
    "  rewards = np.array([])        \n",
    "\n",
    "  # Initialize the variables used to derive % Optimality\n",
    "  count = 0  \n",
    "  optimality = np.array([])\n",
    "\n",
    "  ##### ITERATION THROUGH TIME STEPS #####\n",
    "\n",
    "  for t in range(1,50000):\n",
    "\n",
    "    # Randomly select an arm with probability epsilon \n",
    "    rand = uniform(0,1)\n",
    "    if rand < epsilon:\n",
    "      arm_index = randint(0,n_arms-1)\n",
    "    else:\n",
    "      # Select the arm of maximum mean (reward) with probablity epsilon\n",
    "      arm_index = Q.argmax()\n",
    "    \n",
    "    # Update the reward and append it to Rewards for plotting the evolution\n",
    "    reward = np.random.normal(means[arm_index],1,1)[0]\n",
    "    rewards = np.append(rewards, reward)\n",
    "\n",
    "    # Update % Optimality and append it to Optimality for plotting the evolution\n",
    "    if arm_index == max_index:\n",
    "      count += 1\n",
    "    percent_optimality = count/(t)*100\n",
    "    optimality = np.append(optimality, percent_optimality)\n",
    "\n",
    "    # Update the parameters N,Q\n",
    "    N[arm_index] += 1\n",
    "    Q[arm_index] = Q[arm_index] + (reward - Q[arm_index])/N[arm_index]\n",
    "   \n",
    "  return rewards, optimality\n",
    "\n",
    "''' Averaging over different instances of the bandit problem '''\n",
    "\n",
    "# An iterative averaging technique is used to save comp cost\n",
    "\n",
    "def epsilon_greedy_run(n_arms,epsilon,n_instance):\n",
    "  rewards, optimality = epsilon_greedy(n_arms,epsilon)\n",
    "  for i in range(2,n_instance+1):\n",
    "    rewards_, optimality_ = epsilon_greedy(n_arms,epsilon)\n",
    "    for j in range(len(rewards)):\n",
    "      rewards[j] = rewards[j] + (rewards_[j] - rewards[j])/i\n",
    "      optimality[j] = optimality[j] + (optimality_[j] - optimality[j])/i\n",
    "      \n",
    "  return rewards, optimality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SOFTMAX....###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gibbs(x,beta):\n",
    "\n",
    "    \"\"\" Return the gibbs distribtuion for each sets of scores in x \"\"\"\n",
    "    return np.exp(x/beta) / np.sum(np.exp(x/beta), axis=0)\n",
    "\n",
    "def softmax(n_arms, beta):\n",
    "\n",
    "  \"\"\" Function to implement the softmax action selection method for the bandit problem \"\"\"\n",
    "\n",
    "  # Mean of each of the arms selected from a normal distrubtion of mean 0, variance 1\n",
    "  means = np.random.normal(0,1,n_arms)\n",
    "\n",
    "  # Arm with maximum mean or reward (used to derive % Optimality)\n",
    "  max_index = means.argmax()  \n",
    "\n",
    "  # Initialize parameters \n",
    "  Q = np.zeros(n_arms)\n",
    "  N = np.ones(n_arms)\n",
    "  H = np.zeros(n_arms)\n",
    "\n",
    "  ### Randomly pick an arm ###\n",
    "  arm_index = randint(0, n_arms-1)            \n",
    "  Q[arm_index] = np.random.normal(means[arm_index], 1, 1)[0]\n",
    "\n",
    "  # List to keep track of all the rewards\n",
    "  rewards = np.array([])\n",
    "  \n",
    "  # Obtain probability vector pi from the gibbs ditribution\n",
    "  pi = gibbs(Q, beta)\n",
    "\n",
    "  # Initialize the variables used to derive % Optimality\n",
    "  count = 0  \n",
    "  optimality = np.array([])\n",
    "\n",
    "  for t in range(1,50000):\n",
    "\n",
    "    # Select an arm by sampling from the softmax distribution\n",
    "    arm_index = np.random.choice(len(H),1, p = pi)[0]\n",
    "    reward = np.random.normal(means[arm_index],1,1)[0]\n",
    "\n",
    "    # Update the reward to the array rewards for plotting\n",
    "    rewards = np.append(rewards, reward)\n",
    "\n",
    "    # Update rules for N, Q\n",
    "    N[arm_index] += 1\n",
    "    Q[arm_index] = Q[arm_index] + (reward - Q[arm_index])/N[arm_index]\n",
    "\n",
    "    # Update the probability vector \n",
    "    pi = gibbs(Q, beta)\n",
    "\n",
    "    # Update %Optimality to the array optimality for plotting\n",
    "    if arm_index == max_index:\n",
    "      count += 1\n",
    "    percent_optimality = count/(t)*100\n",
    "    optimality = np.append(optimality, percent_optimality)\n",
    "\n",
    "  return rewards, optimality\n",
    "\n",
    "''' Averaging over different instances of the bandit problem '''\n",
    "\n",
    "# An iterative averaging technique is used to save comp cost\n",
    "\n",
    "def softmax_run(n_arms,beta,n_instance):\n",
    "  rewards, optimality = softmax(n_arms,beta)\n",
    "  for i in range(2,n_instance+1):\n",
    "    rewards_, optimality_ = softmax(n_arms,beta)\n",
    "    for j in range(len(rewards)):\n",
    "      rewards[j] = rewards[j] + (rewards_[j] - rewards[j])/i\n",
    "      optimality[j] = optimality[j] + (optimality_[j] - optimality[j])/i\n",
    "      \n",
    "  return rewards, optimality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  UCB1 ......###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ucb(n_arms,c):\n",
    "  \n",
    "  ''' Function to implement the UCB1 algorithm for the bandit problem... '''\n",
    "\n",
    "  # Mean of each of the arms selected from a normal distrubtion of mean 0, variance 1\n",
    "  means = np.random.normal(0,1,n_arms)  \n",
    "\n",
    "  # Arm with maximum mean or reward (used to derive % Optimality)\n",
    "  max_index = means.argmax()  \n",
    "\n",
    "  # Initialize parameters \n",
    "  Q = np.zeros(n_arms)  \n",
    "  N = np.ones(n_arms)                   \n",
    "\n",
    "  ### Randomly pick an arm ###\n",
    "  arm_index = randint(0, n_arms-1)            \n",
    "  Q[arm_index] = np.random.normal(means[arm_index], 1, 1)[0]\n",
    "\n",
    "  # List to keep track of all the rewards\n",
    "  rewards = np.array([])        \n",
    "\n",
    "  # Initialize the variables used to derive % Optimality\n",
    "  count = 0  \n",
    "  optimality = np.array([])\n",
    "\n",
    "  ##### ITERATION THROUGH TIME STEPS #####\n",
    "\n",
    "  for t in range(1,50000):\n",
    "\n",
    "\n",
    "    arm_index = (Q+c*np.sqrt(np.log(t)/N)).argmax()\n",
    "    \n",
    "    # Update the reward and append it to Rewards for plotting the evolution\n",
    "    reward = np.random.normal(means[arm_index],1,1)[0]\n",
    "    rewards = np.append(rewards, reward)\n",
    "\n",
    "    # Update % Optimality and append it to Optimality for plotting the evolution\n",
    "    if arm_index == max_index:\n",
    "      count += 1\n",
    "    percent_optimality = count/(t)*100\n",
    "    optimality = np.append(optimality, percent_optimality)\n",
    "\n",
    "    # Update the parameters N,Q\n",
    "    N[arm_index] += 1\n",
    "    Q[arm_index] = Q[arm_index] + (reward - Q[arm_index])/N[arm_index]\n",
    "   \n",
    "  return rewards, optimality\n",
    "\n",
    "''' Averaging over different instances of the bandit problem... '''\n",
    "\n",
    "# An iterative averaging technique is used to save comp cost....\n",
    "\n",
    "def ucb_run(n_arms,c,n_instance):\n",
    "  rewards, optimality = ucb(n_arms,c)\n",
    "  for i in range(2,n_instance+1):\n",
    "    rewards_, optimality_ = ucb(n_arms,c)\n",
    "    for j in range(len(rewards)):\n",
    "      rewards[j] = rewards[j] + (rewards_[j] - rewards[j])/i\n",
    "      optimality[j] = optimality[j] + (optimality_[j] - optimality[j])/i\n",
    "      \n",
    "  return rewards, optimality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### MEA .....###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pac(n_arms, epsilon, delta):\n",
    "\n",
    "  ''' Function to implement the MEA algorithm for the bandit problem '''\n",
    " \n",
    "  # Scaling of epsilon and delta  \n",
    "  epsilon = epsilon/4\n",
    "  delta = delta/2\n",
    "\n",
    "  # Mean of each of the arms selected from a normal distrubtion of mean 0, variance 1  \n",
    "  means = np.random.normal(0,1,n_arms)\n",
    "  Q = [np.random.normal(means[i],1,1)[0] for i in range(n_arms)]\n",
    "  \n",
    "  # List to keep track of all the rewards\n",
    "  rewards = []\n",
    "\n",
    "  # Arm with maximum mean or reward (used to derive % Optimality)\n",
    "  optimal_index = means.argmax()\n",
    "  optimal_reward = max(means)\n",
    "  optimality = []\n",
    "\n",
    "  # Array ref has value 1 at the index of best arm and 0 at other indices (used to derive % Optimality)\n",
    "  ref = np.zeros(n_arms)\n",
    "  ref[optimal_index] = 1  \n",
    "\n",
    "  # Initialize variables\n",
    "  l = 1\n",
    "  count = 0\n",
    "  \n",
    "  # Repeat until there is only one arm in the sample space S_l\n",
    "  while len(Q) >= 1:                \n",
    "\n",
    "    l_k = int((1/(epsilon/2)**2)*log(3/delta))\n",
    "    \n",
    "    # Sample each arm l_k number of times\n",
    "    for i in range(2,l_k+1):\n",
    "      for j in range(len(Q)):\n",
    "        Q[j] = Q[j] + (np.random.normal(means[j],1,1)[0]-Q[j])/i\n",
    "      rewards.append(mean(Q))\n",
    "    \n",
    "    # Compute the median of Q and remove the arms which have value less than that of the median\n",
    "    means = [x for _,x in sorted(zip(Q,means))]\n",
    "    ref = [x for _,x in sorted(zip(Q,ref))]\n",
    "    Q.sort()\n",
    "    med = median(Q)\n",
    "    Q = [el for el in Q if el>med]\n",
    "    means = means[-len(Q):]\n",
    "    ref = ref[-len(Q):]\n",
    "    \n",
    "    # Calculation of %Optimal action\n",
    "    # The algo is optimal throughout if it has the best arm in each of the set S_l \n",
    "    \n",
    "    if sum(ref) == 1:   \n",
    "      count += 1\n",
    "    optimality.append(count*100/l)\n",
    "\n",
    "    # Update rules for epsilon and delta\n",
    "    epsilon = 0.75*epsilon\n",
    "    delta = 0.5*delta\n",
    "    \n",
    "    l += 1\n",
    "\n",
    "  return rewards, optimality\n",
    "\n",
    "''' Averaging over different instances of the bandit problem '''\n",
    "\n",
    "# An iterative averaging technique is used to save comp cost\n",
    "\n",
    "def pac_run(n_arms,epsilon,delta,n_instance):\n",
    "  rewards, optimality = pac(n_arms,epsilon,delta)\n",
    "  for i in range(2,n_instance+1):\n",
    "    rewards_, optimality_ = pac(n_arms,epsilon,delta)\n",
    "    for j in range(len(rewards)):\n",
    "      rewards[j] = rewards[j] + (rewards_[j] - rewards[j])/i\n",
    "    for k in range(len(optimality)):\n",
    "      optimality[k] = optimality[k] + (optimality_[k] - optimality[k])/i\n",
    "      \n",
    "  return rewards, optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all the four algos for different with n_arms = 1000, n_instance = 2000\n",
    "\n",
    "### EXPECTED RUN TIME is approx 3 hrs ###\n",
    "\n",
    "t1 = time.time()\n",
    "reward_pac, optimality_pac = pac_run(1000,2,0.8,2000)\n",
    "reward_ucb, optimality_ucb = ucb_run(1000,2,2000)\n",
    "reward_egreedy, optimality_egreedy = epsilon_greedy_run(1000,0.1,2000)\n",
    "reward_softmax, optimality_softmax = softmax_run(1000,0.1,2000)\n",
    "t2 = time.time()\n",
    "print(\"Execution time: \"+str(int(t2-t1))+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward comparison MEA vs UCB1 vs epsilon-greedy vs softmax\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Reward comparison', fontsize = 16)\n",
    "plt.plot(reward_pac,linewidth = 6) \n",
    "plt.plot(reward_ucb)\n",
    "plt.plot(reward_egreedy)\n",
    "plt.plot(reward_softmax)\n",
    "plt.xlabel('Steps', fontsize = 14)\n",
    "plt.ylabel('Average reward', fontsize = 14)\n",
    "plt.legend(['MEA epsilon=2, delta=0.8','UCB1 c=2','greedy-epsilon=0.1','softmax beta=0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Optimal action comparison UCB1 vs epsilon-greedy vs softmax\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Optimality comparison', fontsize = 16)\n",
    "plt.plot(optimality_ucb)\n",
    "plt.plot(optimality_egreedy)\n",
    "plt.plot(optimality_softmax)\n",
    "plt.xlabel('Steps', fontsize = 14)\n",
    "plt.ylabel('% Optimal action', fontsize = 14)\n",
    "plt.legend(['UCB1 c=2','greedy-epsilon=0.1','softmax beta=0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Optimal action of MEA for 1000 arms\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Performance of MEA', fontsize = 16)\n",
    "plt.plot(optimality_pac)\n",
    "plt.xlabel('Steps', fontsize = 14)\n",
    "plt.ylabel('% Optimal action', fontsize = 14)\n",
    "plt.legend(['MEA epsilon=2,delta=0.8'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
